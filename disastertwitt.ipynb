{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real or Not? NLP with Disaster Tweets\n",
    "Predict which Tweets are about real disasters and which ones are not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the libraries and loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# For notebook plotting\n",
    "%matplotlib inline\n",
    "\n",
    "# Standard packages\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import pickle\n",
    "# import sqlite3\n",
    "\n",
    "# nltk for preprocessing of text data\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# sklearn for preprocessing and machine learning models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# XGBoost for Machine Learning (Gradient Boosting Machine (GBM))\n",
    "import xgboost as xgb\n",
    "\n",
    "# Keras for neural networks\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.metrics import accuracy_score,classification_report, recall_score, precision_score\n",
    "from sklearn.metrics import confusion_matrix,mean_absolute_error,mean_squared_error, f1_score\n",
    "\n",
    "\n",
    "# Random seeds for consistent results\n",
    "from tensorflow import set_random_seed\n",
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "set_random_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:\\\\Users\\\\user\\\\Desktop\\\\disaster twitter using nlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "\n",
    "train_df= pd.read_csv('train.csv')\n",
    "test_df=pd.read_csv('test.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Raw Dataframe:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train Raw Dataframe:')\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Raw Dataframe:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Test Raw Dataframe:')\n",
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'keyword', 'location', 'text', 'target'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To know column or attribute names in train data\n",
    "train_df.columns   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'keyword', 'location', 'text'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To know column or attribute names in test_df \n",
    "test_df.columns   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing The first step should be to check the shape of the dataframe and then check the number of null values in each column.\n",
    "\n",
    "In this way we can get an idea of the redundant columns in the data frame depending on which columns have the highest number of null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the train dataframe is (7613, 5)\n",
      "The number of nulls in each column are \n",
      " id             0\n",
      "keyword       61\n",
      "location    2533\n",
      "text           0\n",
      "target         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the train dataframe is\",train_df.shape)\n",
    "print(\"The number of nulls in each column are \\n\", train_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the test dataframe is (3263, 4)\n",
      "The number of nulls in each column are \n",
      " id             0\n",
      "keyword       26\n",
      "location    1105\n",
      "text           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the test dataframe is\",test_df.shape)\n",
    "print(\"The number of nulls in each column are \\n\", test_df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete the columns in train data and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1\n",
       "1             Forest fire near La Ronge Sask. Canada       1\n",
       "2  All residents asked to 'shelter in place' are ...       1\n",
       "3  13,000 people receive #wildfires evacuation or...       1\n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train_df['id']\n",
    "del train_df['keyword']\n",
    "del train_df['location']\n",
    "#del train_df['target']\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                 Just happened a terrible car crash\n",
       "1  Heard about #earthquake is different cities, s...\n",
       "2  there is a forest fire at spot pond, geese are...\n",
       "3           Apocalypse lighting. #Spokane #wildfires\n",
       "4      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del test_df['id']\n",
    "del test_df['keyword']\n",
    "del test_df['location']\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "We created a preprocessor class to perform all steps that need to be performed before the text data can be vectorized. These preprocessing steps include:\n",
    "\n",
    "Tokenization Removing stop words Stemming Transform the tokens back to one string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing for train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessor:\n",
    "    '''\n",
    "    Easily performs all the standard preprocessing steps\n",
    "    like removing stopwords, stemming, etc.\n",
    "    Only input that you need to provide is the dataframe and column name for the tweets\n",
    "    '''\n",
    "    def __init__(self, train_df, column_name):\n",
    "        self.data = train_df\n",
    "        self.conversations = list(self.data[column_name])\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        self.stemmer = SnowballStemmer(\"english\")\n",
    "        self.preprocessed = []\n",
    "        \n",
    "    def tokenize(self, sentence):\n",
    "        '''\n",
    "        Splits up words and makes a list of all words in the tweet\n",
    "        '''\n",
    "        tokenized_sentence = word_tokenize(sentence)\n",
    "        return tokenized_sentence\n",
    "            \n",
    "    def remove_stopwords(self, sentence):\n",
    "        '''Removes stopwords like 'a', 'the', 'and', etc.'''\n",
    "        filtered_sentence = []\n",
    "        for w in sentence:\n",
    "            if w not in self.stopwords and len(w) > 1 and w[:2] != '//' and w != 'https': \n",
    "                filtered_sentence.append(w)\n",
    "        return filtered_sentence\n",
    "    \n",
    "    def stem(self, sentence):\n",
    "        '''\n",
    "        Stems certain words to their root form.\n",
    "        For example, words like 'computer', 'computation'\n",
    "        all get truncated to 'comput'\n",
    "        '''\n",
    "        return [self.stemmer.stem(word) for word in sentence]\n",
    "    \n",
    "    def join_to_string(self, sentence):\n",
    "        '''\n",
    "        Joins the tokenized words to one string.\n",
    "        '''\n",
    "        return ' '.join(sentence)\n",
    "    \n",
    "    def full_preprocess(self, n_rows=None):\n",
    "        '''\n",
    "        Preprocess a selected number of rows and\n",
    "        connects them back to strings\n",
    "        '''\n",
    "        # If nothing is given do it for the whole dataset\n",
    "        if n_rows == None:\n",
    "            n_rows = len(self.data)\n",
    "            \n",
    "        # Perform preprocessing\n",
    "        for i in range(n_rows):\n",
    "            tweet = self.conversations[i]\n",
    "            tokenized = self.tokenize(tweet)\n",
    "            cleaned = self.remove_stopwords(tokenized)\n",
    "            stemmed = self.stem(cleaned)\n",
    "            joined = self.join_to_string(stemmed)\n",
    "            self.preprocessed.append(joined)\n",
    "        return self.preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text and put it in a new column\n",
    "preprocessor = PreProcessor(train_df, 'text')\n",
    "train_df['cleaned_text'] = preprocessor.full_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_df['text']    # deleted text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>our deed reason earthquak may allah forgiv us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>all resid ask shelter place notifi offic no ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>13,000 peopl receiv wildfir evacu order califo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>just got sent photo rubi alaska smoke wildfir ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>rockyfir updat california hwi 20 close direct ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>flood disast heavi rain caus flash flood stree...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>'m top hill see fire wood ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>there 's emerg evacu happen build across street</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>'m afraid tornado come area ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>three peopl die heat wave far</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>haha south tampa get flood hah- wait second li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>rain flood florida tampabay tampa 18 19 day ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>flood bago myanmar we arriv bago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>damag school bus 80 multi car crash break</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>what 's man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>love fruit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>summer love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>my car fast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>what goooooooaaaaaal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>ridicul ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>london cool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>love ski</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>what wonder day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>looooool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>no way ... ca n't eat shit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>was nyc last week</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>love girlfriend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>cooool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>do like pasta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7583</th>\n",
       "      <td>1</td>\n",
       "      <td>pic 16yr old pkk suicid bomber deton bomb turk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7584</th>\n",
       "      <td>0</td>\n",
       "      <td>these box readi explod explod kitten final arr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7585</th>\n",
       "      <td>1</td>\n",
       "      <td>calgari polic flood road closur calgari http</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7586</th>\n",
       "      <td>1</td>\n",
       "      <td>sismo detectado japì_n 15:41:07 seismic intens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7587</th>\n",
       "      <td>0</td>\n",
       "      <td>siren everywher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7588</th>\n",
       "      <td>1</td>\n",
       "      <td>break isi claim respons mosqu attack saudi ara...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7589</th>\n",
       "      <td>1</td>\n",
       "      <td>omg earthquak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7590</th>\n",
       "      <td>1</td>\n",
       "      <td>sever weather bulletin no for typhoon ûï hann...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7591</th>\n",
       "      <td>1</td>\n",
       "      <td>heat wave warn aa ayyo dei just plan visit fri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7592</th>\n",
       "      <td>1</td>\n",
       "      <td>an is group suicid bomber deton explosives-pac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7593</th>\n",
       "      <td>0</td>\n",
       "      <td>heard realli loud bang everyon asleep great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7594</th>\n",
       "      <td>1</td>\n",
       "      <td>gas thing explod heard scream whole street sme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7595</th>\n",
       "      <td>1</td>\n",
       "      <td>nws flash flood warn continu shelbi counti 08:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7596</th>\n",
       "      <td>1</td>\n",
       "      <td>rt livingsaf nws issu sever thunderstorm warn ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7597</th>\n",
       "      <td>1</td>\n",
       "      <td>mh370 aircraft debri found la reunion miss mal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7598</th>\n",
       "      <td>1</td>\n",
       "      <td>father-of-thre lost control car after overtak ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7599</th>\n",
       "      <td>1</td>\n",
       "      <td>1.3 earthquak 9km ssw of anza california iphon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7600</th>\n",
       "      <td>1</td>\n",
       "      <td>evacu order lift town roosevelt http http</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7601</th>\n",
       "      <td>1</td>\n",
       "      <td>break la refugio oil spill may costlier bigger...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7602</th>\n",
       "      <td>1</td>\n",
       "      <td>siren went n't forney tornado warn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7603</th>\n",
       "      <td>1</td>\n",
       "      <td>offici say quarantin place alabama home possib...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7604</th>\n",
       "      <td>1</td>\n",
       "      <td>worldnew fallen powerlin link tram updat fire ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7605</th>\n",
       "      <td>1</td>\n",
       "      <td>flip side 'm walmart bomb everyon evacu stay t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7606</th>\n",
       "      <td>1</td>\n",
       "      <td>suicid bomber kill 15 saudi secur site mosqu r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7607</th>\n",
       "      <td>1</td>\n",
       "      <td>stormchas violent record break ef-5 el reno ok...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>1</td>\n",
       "      <td>two giant crane hold bridg collaps nearbi home...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>1</td>\n",
       "      <td>aria_ahrari thetawniest the control wild fire ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>1</td>\n",
       "      <td>m1.94 01:04 utc 5km volcano hawaii http</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>1</td>\n",
       "      <td>polic investig e-bik collid car littl portug e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>1</td>\n",
       "      <td>the latest more home raze northern california ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      target                                       cleaned_text\n",
       "0          1      our deed reason earthquak may allah forgiv us\n",
       "1          1               forest fire near la rong sask canada\n",
       "2          1  all resid ask shelter place notifi offic no ev...\n",
       "3          1  13,000 peopl receiv wildfir evacu order califo...\n",
       "4          1  just got sent photo rubi alaska smoke wildfir ...\n",
       "5          1  rockyfir updat california hwi 20 close direct ...\n",
       "6          1  flood disast heavi rain caus flash flood stree...\n",
       "7          1                      'm top hill see fire wood ...\n",
       "8          1    there 's emerg evacu happen build across street\n",
       "9          1                    'm afraid tornado come area ...\n",
       "10         1                      three peopl die heat wave far\n",
       "11         1  haha south tampa get flood hah- wait second li...\n",
       "12         1  rain flood florida tampabay tampa 18 19 day ve...\n",
       "13         1                   flood bago myanmar we arriv bago\n",
       "14         1          damag school bus 80 multi car crash break\n",
       "15         0                                        what 's man\n",
       "16         0                                         love fruit\n",
       "17         0                                        summer love\n",
       "18         0                                        my car fast\n",
       "19         0                               what goooooooaaaaaal\n",
       "20         0                                        ridicul ...\n",
       "21         0                                        london cool\n",
       "22         0                                           love ski\n",
       "23         0                                    what wonder day\n",
       "24         0                                           looooool\n",
       "25         0                         no way ... ca n't eat shit\n",
       "26         0                                  was nyc last week\n",
       "27         0                                    love girlfriend\n",
       "28         0                                             cooool\n",
       "29         0                                      do like pasta\n",
       "...      ...                                                ...\n",
       "7583       1  pic 16yr old pkk suicid bomber deton bomb turk...\n",
       "7584       0  these box readi explod explod kitten final arr...\n",
       "7585       1       calgari polic flood road closur calgari http\n",
       "7586       1  sismo detectado japì_n 15:41:07 seismic intens...\n",
       "7587       0                                    siren everywher\n",
       "7588       1  break isi claim respons mosqu attack saudi ara...\n",
       "7589       1                                      omg earthquak\n",
       "7590       1  sever weather bulletin no for typhoon ûï hann...\n",
       "7591       1  heat wave warn aa ayyo dei just plan visit fri...\n",
       "7592       1  an is group suicid bomber deton explosives-pac...\n",
       "7593       0        heard realli loud bang everyon asleep great\n",
       "7594       1  gas thing explod heard scream whole street sme...\n",
       "7595       1  nws flash flood warn continu shelbi counti 08:...\n",
       "7596       1  rt livingsaf nws issu sever thunderstorm warn ...\n",
       "7597       1  mh370 aircraft debri found la reunion miss mal...\n",
       "7598       1  father-of-thre lost control car after overtak ...\n",
       "7599       1  1.3 earthquak 9km ssw of anza california iphon...\n",
       "7600       1          evacu order lift town roosevelt http http\n",
       "7601       1  break la refugio oil spill may costlier bigger...\n",
       "7602       1                 siren went n't forney tornado warn\n",
       "7603       1  offici say quarantin place alabama home possib...\n",
       "7604       1  worldnew fallen powerlin link tram updat fire ...\n",
       "7605       1  flip side 'm walmart bomb everyon evacu stay t...\n",
       "7606       1  suicid bomber kill 15 saudi secur site mosqu r...\n",
       "7607       1  stormchas violent record break ef-5 el reno ok...\n",
       "7608       1  two giant crane hold bridg collaps nearbi home...\n",
       "7609       1  aria_ahrari thetawniest the control wild fire ...\n",
       "7610       1            m1.94 01:04 utc 5km volcano hawaii http\n",
       "7611       1  polic investig e-bik collid car littl portug e...\n",
       "7612       1  the latest more home raze northern california ...\n",
       "\n",
       "[7613 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df    # after cleaning in the train dataframe  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessor:\n",
    "    '''\n",
    "    Easily performs all the standard preprocessing steps\n",
    "    like removing stopwords, stemming, etc.\n",
    "    Only input that you need to provide is the dataframe and column name for the tweets\n",
    "    '''\n",
    "    def __init__(self, test_df, column_name):\n",
    "        self.data = test_df\n",
    "        self.conversations = list(self.data[column_name])\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        self.stemmer = SnowballStemmer(\"english\")\n",
    "        self.preprocessed = []\n",
    "        \n",
    "    def tokenize(self, sentence):\n",
    "        '''\n",
    "        Splits up words and makes a list of all words in the tweet\n",
    "        '''\n",
    "        tokenized_sentence = word_tokenize(sentence)\n",
    "        return tokenized_sentence\n",
    "            \n",
    "    def remove_stopwords(self, sentence):\n",
    "        '''Removes stopwords like 'a', 'the', 'and', etc.'''\n",
    "        filtered_sentence = []\n",
    "        for w in sentence:\n",
    "            if w not in self.stopwords and len(w) > 1 and w[:2] != '//' and w != 'https': \n",
    "                filtered_sentence.append(w)\n",
    "        return filtered_sentence\n",
    "    \n",
    "    def stem(self, sentence):\n",
    "        '''\n",
    "        Stems certain words to their root form.\n",
    "        For example, words like 'computer', 'computation'\n",
    "        all get truncated to 'comput'\n",
    "        '''\n",
    "        return [self.stemmer.stem(word) for word in sentence]\n",
    "    \n",
    "    def join_to_string(self, sentence):\n",
    "        '''\n",
    "        Joins the tokenized words to one string.\n",
    "        '''\n",
    "        return ' '.join(sentence)\n",
    "    \n",
    "    def full_preprocess(self, n_rows=None):\n",
    "        '''\n",
    "        Preprocess a selected number of rows and\n",
    "        connects them back to strings\n",
    "        '''\n",
    "        # If nothing is given do it for the whole dataset\n",
    "        if n_rows == None:\n",
    "            n_rows = len(self.data)\n",
    "            \n",
    "        # Perform preprocessing\n",
    "        for i in range(n_rows):\n",
    "            tweet = self.conversations[i]\n",
    "            tokenized = self.tokenize(tweet)\n",
    "            cleaned = self.remove_stopwords(tokenized)\n",
    "            stemmed = self.stem(cleaned)\n",
    "            joined = self.join_to_string(stemmed)\n",
    "            self.preprocessed.append(joined)\n",
    "        return self.preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text and put it in a new column\n",
    "preprocessor = PreProcessor(test_df, 'text')\n",
    "test_df['cleaned_text'] = preprocessor.full_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test_df['text']    # deleted text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>just happen terribl car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>heard earthquak differ citi stay safe everyon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>forest fire spot pond gees flee across street ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>apocalyps light spokan wildfir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>typhoon soudelor kill 28 china taiwan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>we re shake ... it 's earthquak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>they 'd probabl still show life arsenal yester...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hey how</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>what nice hat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fuck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>no n't like cold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>nooooooooo do n't</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>no n't tell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>what</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>awesom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>birmingham wholesal market ablaz bbc news fire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>sunkxssedharri wear short race ablaz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>previouslyondoyintv toke makinwaûª marriag cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>check http http http http nsfw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>psa iûªm split person techi follow ablaze_co ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>bewar world ablaz sierra leon amp guap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>burn man ablaz turban diva http via etsi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>not diss song peopl take thing run smh 's eye ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>rape victim die set ablaz 16-year-old girl die...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>set myself ablaz http</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ctvtoronto bin front field hous wer set ablaz ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>nowplay alfon ablaz 2015 pul radio pulsradio http</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>burn rahm let 's hope citi hall build giant wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>philippaeilhart dhublath hurt eye ablaz insult...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>accid clear paturnpik patp eb pa-18 cranberri ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3233</th>\n",
       "      <td>wreckag conclus confirm from mh370 malaysia pm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3234</th>\n",
       "      <td>scienc now piec wreckag flight mh370 confirm r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3235</th>\n",
       "      <td>wreckag conclus confirm from mh370 malaysia pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3236</th>\n",
       "      <td>wreckag conclus confirm from mh370 malaysia pm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3237</th>\n",
       "      <td>top stori wreckag mh370 offici confirm janisct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3238</th>\n",
       "      <td>wreckag conclus confirm from mh370 malaysia pm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3239</th>\n",
       "      <td>wreckag conclus confirm from mh370 malaysia pm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3240</th>\n",
       "      <td>rt australian debri found indian ocean island ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3241</th>\n",
       "      <td>cramer iger 's word wreck disney 's stock http</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3242</th>\n",
       "      <td>almost *wrecked* van day guy yeah brake amp al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3243</th>\n",
       "      <td>what manner human would parcel babi though wre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3244</th>\n",
       "      <td>nathan26_rfc thought said saturday night near ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3245</th>\n",
       "      <td>wan na eas mind make feel alright.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3246</th>\n",
       "      <td>yakuboob think deactiv notif aid tesco wreck lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3247</th>\n",
       "      <td>rt cnbc word disney ceo bob iger wreck disney ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3248</th>\n",
       "      <td>smackdown tyme put good mood sinc got wreck smh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3249</th>\n",
       "      <td>thrillhho jsyk n't stop think abt remus slump ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3250</th>\n",
       "      <td>stighefootbal begov garbag he got wreck red bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3251</th>\n",
       "      <td>wreck today got hattrick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3252</th>\n",
       "      <td>ebola ebolaoutbreak ebola virus birmingham ala...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3253</th>\n",
       "      <td>malaysian pm confirm debri miss flight mh370 http</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3254</th>\n",
       "      <td>offici alabama home quarantin possibl ebola ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255</th>\n",
       "      <td>see 16yr old pkk suicid bomber deton bomb turk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3256</th>\n",
       "      <td>to confer attende the blue line airport derail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3257</th>\n",
       "      <td>the death toll is-suicid car bomb ypg posit vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>earthquak safeti los angel ûò safeti fasten xrwn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>storm ri wors last hurrican my citi amp 3other...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>green line derail chicago http</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>meg issu hazard weather outlook hwo http</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>cityofcalgari activ municip emerg plan yycstorm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           cleaned_text\n",
       "0                         just happen terribl car crash\n",
       "1         heard earthquak differ citi stay safe everyon\n",
       "2     forest fire spot pond gees flee across street ...\n",
       "3                        apocalyps light spokan wildfir\n",
       "4                 typhoon soudelor kill 28 china taiwan\n",
       "5                       we re shake ... it 's earthquak\n",
       "6     they 'd probabl still show life arsenal yester...\n",
       "7                                               hey how\n",
       "8                                         what nice hat\n",
       "9                                                  fuck\n",
       "10                                     no n't like cold\n",
       "11                                    nooooooooo do n't\n",
       "12                                          no n't tell\n",
       "13                                                 what\n",
       "14                                               awesom\n",
       "15    birmingham wholesal market ablaz bbc news fire...\n",
       "16                 sunkxssedharri wear short race ablaz\n",
       "17    previouslyondoyintv toke makinwaûª marriag cr...\n",
       "18                       check http http http http nsfw\n",
       "19    psa iûªm split person techi follow ablaze_co ...\n",
       "20               bewar world ablaz sierra leon amp guap\n",
       "21             burn man ablaz turban diva http via etsi\n",
       "22    not diss song peopl take thing run smh 's eye ...\n",
       "23    rape victim die set ablaz 16-year-old girl die...\n",
       "24                                set myself ablaz http\n",
       "25    ctvtoronto bin front field hous wer set ablaz ...\n",
       "26    nowplay alfon ablaz 2015 pul radio pulsradio http\n",
       "27    burn rahm let 's hope citi hall build giant wo...\n",
       "28    philippaeilhart dhublath hurt eye ablaz insult...\n",
       "29    accid clear paturnpik patp eb pa-18 cranberri ...\n",
       "...                                                 ...\n",
       "3233  wreckag conclus confirm from mh370 malaysia pm...\n",
       "3234  scienc now piec wreckag flight mh370 confirm r...\n",
       "3235     wreckag conclus confirm from mh370 malaysia pm\n",
       "3236  wreckag conclus confirm from mh370 malaysia pm...\n",
       "3237  top stori wreckag mh370 offici confirm janisct...\n",
       "3238  wreckag conclus confirm from mh370 malaysia pm...\n",
       "3239  wreckag conclus confirm from mh370 malaysia pm...\n",
       "3240  rt australian debri found indian ocean island ...\n",
       "3241     cramer iger 's word wreck disney 's stock http\n",
       "3242  almost *wrecked* van day guy yeah brake amp al...\n",
       "3243  what manner human would parcel babi though wre...\n",
       "3244  nathan26_rfc thought said saturday night near ...\n",
       "3245                 wan na eas mind make feel alright.\n",
       "3246   yakuboob think deactiv notif aid tesco wreck lol\n",
       "3247  rt cnbc word disney ceo bob iger wreck disney ...\n",
       "3248    smackdown tyme put good mood sinc got wreck smh\n",
       "3249  thrillhho jsyk n't stop think abt remus slump ...\n",
       "3250  stighefootbal begov garbag he got wreck red bu...\n",
       "3251                           wreck today got hattrick\n",
       "3252  ebola ebolaoutbreak ebola virus birmingham ala...\n",
       "3253  malaysian pm confirm debri miss flight mh370 http\n",
       "3254  offici alabama home quarantin possibl ebola ca...\n",
       "3255  see 16yr old pkk suicid bomber deton bomb turk...\n",
       "3256  to confer attende the blue line airport derail...\n",
       "3257  the death toll is-suicid car bomb ypg posit vi...\n",
       "3258  earthquak safeti los angel ûò safeti fasten xrwn\n",
       "3259  storm ri wors last hurrican my citi amp 3other...\n",
       "3260                     green line derail chicago http\n",
       "3261           meg issu hazard weather outlook hwo http\n",
       "3262    cityofcalgari activ municip emerg plan yycstorm\n",
       "\n",
       "[3263 rows x 1 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df    # after cleaning in the test dataframe  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train dataframe into train and test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We split the train data into a training set, and test set. This is crucial for training and evaluation of good machine learning models.\n",
    "\n",
    "The data will be splitted into 80/20. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>our deed reason earthquak may allah forgiv us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>all resid ask shelter place notifi offic no ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>13,000 peopl receiv wildfir evacu order califo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>just got sent photo rubi alaska smoke wildfir ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                       cleaned_text\n",
       "0       1      our deed reason earthquak may allah forgiv us\n",
       "1       1               forest fire near la rong sask canada\n",
       "2       1  all resid ask shelter place notifi offic no ev...\n",
       "3       1  13,000 peopl receiv wildfir evacu order califo...\n",
       "4       1  just got sent photo rubi alaska smoke wildfir ..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df[\"cleaned_text\"]\n",
    "y = train_df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613,)\n",
      "(7613,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val, y_train, y_val = train_test_split(X, y, test_size=0.2,random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6090,)\n",
      "(6090,)\n",
      "(1523,)\n",
      "(1523,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data vectorization for train dataframe\n",
    "\n",
    "Data vectorization Many machine learning models can only be trained on numerical input in the form of vectors or matrices. To prepare our tweets for the machine learning models we create a term frequency-inverse document frequency (tf-idf) vectorization. The result of this vectorization is a sparse matrix which contains a convenient representation of our tweets.\n",
    "\n",
    "The machine learning will learn which word frequency is important to predict a correct sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create  matrix based on word frequency in tweets\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_val = vectorizer.transform(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 6090 tweets\n",
      "Test size: 1523 tweets\n",
      "Amount of words (columns): 12469 words\n"
     ]
    }
   ],
   "source": [
    "# Print the size of our data\n",
    "\n",
    "print(f'Train size: {X_train.shape[0]} tweets\\n\\\n",
    "Test size: {X_val.shape[0]} tweets\\n\\\n",
    "Amount of words (columns): {X_train.shape[1]} words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data vectorization for test dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df[\"cleaned_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = vectorizer.transform(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multinomial Naive Bayes\n",
    "multi_nb = MultinomialNB()\n",
    "multi_nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_mn = multi_nb.predict(X_train)\n",
    "val_pred_mn = multi_nb.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data\n",
      "f1 score is 0.8655479593950695\n",
      "\n",
      "\n",
      " Validation data\n",
      "f1 score is 0.7533783783783784\n"
     ]
    }
   ],
   "source": [
    "######Getting evaluation metrics and evaluating model performance\n",
    "\n",
    "###### Check results\n",
    "print(\"Train data\")\n",
    "#print(f'Accuracy on training set (MultinomialNB): {round(accuracy_score(y_train, train_pred_mn)*100, 4)}%')\n",
    "print(\"f1 score is\",f1_score(y_train,train_pred_mn,pos_label=1))\n",
    "#print(classification_report(y_train,train_pred_mn,digits=4))\n",
    "#print(confusion_matrix(y_train, train_pred_mn)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\" Validation data\")\n",
    "#print(f'Accuracy on test set (MultinomialNB): {round(accuracy_score(y_val,val_pred_mn)*100, 4)}%')\n",
    "print(\"f1 score is\",f1_score(y_val,val_pred_mn,pos_label=1))\n",
    "#print(classification_report(y_val,val_pred_mn,digits=4))\n",
    "#print(confusion_matrix(y_val, val_pred_mn ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = DecisionTreeClassifier(max_depth=10)\n",
    "estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_dt = estimator.predict(X_train)\n",
    "val_pred_dt = estimator.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data\n",
      "f1 score is 0.6770098730606487\n",
      "\n",
      "\n",
      "validation data\n",
      "f1 score is 0.6436132674664784\n"
     ]
    }
   ],
   "source": [
    "######Getting evaluation metrics and evaluating model performance\n",
    "\n",
    "###### Check results\n",
    "print(\"Train data\")\n",
    "print(\"f1 score is\",f1_score(y_train,train_pred_dt,pos_label=1))\n",
    "#print(accuracy_score(y_train,train_pred_dt))\n",
    "#print(classification_report(y_train,train_pred_dt,digits=4))\n",
    "#print(confusion_matrix(y_train, train_pred_dt))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"validation data\")\n",
    "print(\"f1 score is\",f1_score(y_val,val_pred_dt,pos_label=1))\n",
    "#print(acuracy_score(y_val,val_pred_dt))\n",
    "#print(classification_report(y_val,val_pred_dt,digits=4))\n",
    "#print(confusion_matrix(y_val, val_pred_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBM_model = GradientBoostingClassifier(n_estimators=200,\n",
    "                                       learning_rate=0.1,\n",
    "                                       subsample=0.8, max_depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.1, loss='deviance', max_depth=5,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "                           n_iter_no_change=None, presort='auto',\n",
       "                           random_state=None, subsample=0.8, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time GBM_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_gb=GBM_model.predict(X_train)\n",
    "val_pred_gb=GBM_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data\n",
      "f1 score is 0.8442657638595006\n",
      "\n",
      "\n",
      "validation data\n",
      "f1 score is 0.7337180544105524\n"
     ]
    }
   ],
   "source": [
    "#### check results\n",
    "print(\"Train data\")\n",
    "print(\"f1 score is\",f1_score(y_train,train_pred_gb,pos_label=1))\n",
    "#print(accuracy_score(y_train,train_pred_gb))\n",
    "#print(classification_report(y_train,train_pred_gb,digits=4))\n",
    "#print(confusion_matrix(y_train, train_pred_gb))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"validation data\")\n",
    "print(\"f1 score is\",f1_score(y_val,val_pred_gb,pos_label=1))\n",
    "#print(accuracy_score(y_val,val_pred_gb))\n",
    "#print(classification_report(y_val,val_pred_gb,digits=4))\n",
    "#print(confusion_matrix(y_val, val_pred_gb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=8, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=10,max_depth=8)\n",
    "rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_rf = rf_clf.predict(X_train)\n",
    "val_pred_rf = rf_clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data\n",
      "f1 score is 0.2900713822193381\n",
      "\n",
      "\n",
      "validation data\n",
      "f1 score is 0.25498007968127495\n"
     ]
    }
   ],
   "source": [
    "#### check results\n",
    "print(\"Train data\")\n",
    "print(\"f1 score is\",f1_score(y_train,train_pred_rf,pos_label=1))\n",
    "#print(accuracy_score(y_train,train_pred_rf))\n",
    "#print(classification_report(y_train,train_pred_rf,digits=4))\n",
    "#print(confusion_matrix(y_train, train_pred_rf))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"validation data\")\n",
    "print(\"f1 score is\",f1_score(y_val,val_pred_rf,pos_label=1))\n",
    "#print(accuracy_score(y_val,val_pred_rf))\n",
    "#print(classification_report(y_val,val_pred_rf,digits=4))\n",
    "#print(confusion_matrix(y_val, val_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# grid search cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_jobs=-1, max_features='sqrt') \n",
    " \n",
    "# Use a grid over parameters of interest\n",
    "param_grid = { \n",
    "           \"n_estimators\" : [9, 18, 27, 36],\n",
    "           \"max_depth\" : [2,3,5],\n",
    "           \"min_samples_leaf\" : [2, 4]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_gcv_clf = GridSearchCV(estimator=rfc, param_grid=param_grid,cv=5,\n",
    "                       scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='sqrt',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators='warn', n_jobs=-1,\n",
       "                                              oob_score=False,\n",
       "                                              random_state=None, verbose=0,\n",
       "                                              warm_start=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'max_depth': [2, 3, 5], 'min_samples_leaf': [2, 4],\n",
       "                         'n_estimators': [9, 18, 27, 36]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_gcv_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_gcv_pred_train =rf_gcv_clf.predict(X_train)\n",
    "rf_gcv_pred_val=rf_gcv_clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data\n",
      "f1 score is 0.21768251841929\n",
      "\n",
      "\n",
      "validation data\n",
      "f1 score is 0.22696929238985314\n"
     ]
    }
   ],
   "source": [
    "#### check results\n",
    "print(\"Train data\")\n",
    "print(\"f1 score is\",f1_score(y_train,rf_gcv_pred_train,pos_label=1))\n",
    "#print(accuracy_score(y_train,rf_gcv_pred_train))\n",
    "#print(classification_report(y_train,rf_gcv_pred_train,digits=4))\n",
    "#print(confusion_matrix(y_train, rf_gcv_pred_train))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"validation data\")\n",
    "print(\"f1 score is\",f1_score(y_val,rf_gcv_pred_val,pos_label=1))\n",
    "#print(accuracy_score(y_val,rf_gcv_pred_val))\n",
    "#print(classification_report(y_val,rf_gcv_pred_val,digits=4))\n",
    "#print(confusion_matrix(y_val, rf_gcv_pred_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator so we can easily feed batches of data to the neural network\n",
    "def batch_generator(X, y, batch_size, shuffle):\n",
    "    number_of_batches = X.shape[0]/batch_size\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X[batch_index,:].toarray()\n",
    "        y_batch = y[batch_index]\n",
    "        counter += 1\n",
    "        yield X_batch, y_batch\n",
    "        if (counter == number_of_batches):\n",
    "            if shuffle:\n",
    "                np.random.shuffle(sample_index)\n",
    "            counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\user\\Anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Onehot encoding of target variable\n",
    "\n",
    "# Initialize sklearn's one-hot encoder class\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# One hot encoding for training set\n",
    "integer_encoded_train = np.array(y_train).reshape(len(y_train), 1)\n",
    "onehot_encoded_train = onehot_encoder.fit_transform(integer_encoded_train)\n",
    "\n",
    "# One hot encoding for validation set\n",
    "integer_encoded_val = np.array(y_val).reshape(len(y_val), 1)\n",
    "onehot_encoded_val = onehot_encoder.fit_transform(integer_encoded_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\envs\\py36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/5\n",
      "48/47 [==============================] - 5s 102ms/step - loss: 0.6899 - acc: 0.5771 - val_loss: 0.6859 - val_acc: 0.6123\n",
      "Epoch 2/5\n",
      "48/47 [==============================] - 32s 657ms/step - loss: nan - acc: nan - val_loss: 0.6844 - val_acc: 0.6129\n",
      "Epoch 3/5\n",
      "48/47 [==============================] - 31s 654ms/step - loss: nan - acc: nan - val_loss: 0.6844 - val_acc: 0.6126\n",
      "Epoch 4/5\n",
      "48/47 [==============================] - 30s 619ms/step - loss: nan - acc: nan - val_loss: 0.6844 - val_acc: 0.6126\n",
      "Epoch 5/5\n",
      "48/47 [==============================] - 29s 612ms/step - loss: nan - acc: nan - val_loss: 0.6844 - val_acc: 0.6126\n"
     ]
    }
   ],
   "source": [
    "initializer = keras.initializers.he_normal(seed=seed)\n",
    "activation = keras.activations.elu\n",
    "optimizer = keras.optimizers.Adam(lr=0.0002, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "es = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=4)\n",
    "\n",
    "# Build model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(20, activation=activation, kernel_initializer=initializer, input_dim=X_train.shape[1]))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(2, activation='sigmoid', kernel_initializer=initializer))\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 5\n",
    "batch_size = 128\n",
    "\n",
    "# Fit the model using the batch_generator\n",
    "hist = model.fit_generator(generator=batch_generator(X_train, onehot_encoded_train, batch_size=batch_size, shuffle=True),\n",
    "                           epochs=epochs, validation_data=(X_val, onehot_encoded_val),\n",
    "                           steps_per_epoch=X_train.shape[0]/batch_size, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=multi_nb.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub=pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.drop([\"target\"], axis =1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['target']=prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(\"sample.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction1=GBM_model.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1=pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       0\n",
       "1   2       0\n",
       "2   3       0\n",
       "3   9       0\n",
       "4  11       0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1.drop([\"target\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1['target']=prediction1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2096\n",
       "1    1167\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub1['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1.to_csv(\"sample1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# overall by seeing the performance, Naive bayes giving the best result comparing to other classificatio  models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
